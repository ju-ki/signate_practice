{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# はじめに\n",
    ".ipynbを共有するのは初めてなので，読みにくいのはご了承ください．\n",
    "\n",
    "初日から，LBのF1スコア高くてびっくりしています．\n",
    "\n",
    "スコアが高いbaselineモデルのトピックがすでに上がっていたので，投稿するか迷いましたがせっかくなので共有させてもらえたらと思います．\n",
    "\n",
    "以前のコンペで，bertを使ったのでその時に使用したシンプルなモデルのコードをほとんどそのまま流用しています．\n",
    "\n",
    "詳しい解説は[【PyTorch】BERTのfine-tuningを試してみた(SIGNATE Student Cup 2020)](https://qiita.com/takaito0423/items/bf103898371106a67898)\n",
    "に記載しているので，もしこのモデルを使ってみたい場合は合わせて読んでもらえたらと思います．\n",
    "\n",
    "全然工夫をしていないので，まだまだスコアをあげることができると思うので，何かの参考になれば幸いです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不均衡データ対策\n",
    "ラベルの割合で重み付け．\n",
    "\n",
    "## 学習に時間がかかる\n",
    "学習データの通常メールとスパムメールの割合を10:1になるようにサンプリング → './data/bert_train_data.csv'\n",
    "（データ数が不均衡なので，とりあえずはデータ数を少なくして学習させてみた）\n",
    "\n",
    "'test_data.csv'に'y'のコラム(0埋め)を追加したデータ → './data/bert_test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7 (default, May  7 2020, 12:27:09) \n",
      "[Clang 11.0.3 (clang-1103.0.32.59)]\n",
      "amply==0.1.2\n",
      "appnope==0.1.0\n",
      "attrdict==2.0.1\n",
      "attrs==19.2.0\n",
      "backcall==0.1.0\n",
      "beautifulsoup4==4.8.2\n",
      "bleach==3.1.0\n",
      "boto==2.49.0\n",
      "boto3==1.11.9\n",
      "botocore==1.14.9\n",
      "bs4==0.0.1\n",
      "catboost==0.24.1\n",
      "certifi==2019.9.11\n",
      "chardet==3.0.4\n",
      "click==7.1.2\n",
      "cycler==0.10.0\n",
      "Cython==0.29.15\n",
      "decorator==4.4.0\n",
      "defusedxml==0.6.0\n",
      "docutils==0.15.2\n",
      "dyNET==2.1\n",
      "emoji==0.5.4\n",
      "entrypoints==0.3\n",
      "filelock==3.0.12\n",
      "future==0.18.2\n",
      "gensim==3.8.1\n",
      "get==2019.4.13\n",
      "graphviz==0.14.1\n",
      "idna==2.8\n",
      "ipykernel==5.1.2\n",
      "ipython==7.8.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "jedi==0.15.1\n",
      "Jinja2==2.10.1\n",
      "jmespath==0.9.4\n",
      "joblib==0.14.0\n",
      "jsonschema==3.0.2\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.3.3\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.5.0\n",
      "kiwisolver==1.1.0\n",
      "lightgbm==2.3.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.2.0\n",
      "mecab==0.996\n",
      "mistune==0.8.4\n",
      "mojimoji==0.0.9\n",
      "nagisa==0.2.5\n",
      "nbconvert==5.6.0\n",
      "nbformat==4.4.0\n",
      "notebook==6.0.1\n",
      "numpy==1.17.2\n",
      "opencv-python==4.2.0.34\n",
      "packaging==20.4\n",
      "pandas==1.1.1\n",
      "pandocfilters==1.4.2\n",
      "parso==0.5.1\n",
      "pexpect==4.7.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==6.2.0\n",
      "plotly==4.10.0\n",
      "post==2019.4.13\n",
      "prometheus-client==0.7.1\n",
      "prompt-toolkit==2.0.9\n",
      "ptyprocess==0.6.0\n",
      "public==2019.4.13\n",
      "PuLP==2.3\n",
      "Pygments==2.4.2\n",
      "pyparsing==2.4.2\n",
      "pyrsistent==0.15.4\n",
      "python-dateutil==2.8.0\n",
      "pytz==2019.3\n",
      "pyzmq==18.1.0\n",
      "qtconsole==4.5.5\n",
      "query-string==2019.4.13\n",
      "regex==2020.10.15\n",
      "request==2019.4.13\n",
      "requests==2.22.0\n",
      "retrying==1.3.3\n",
      "s3transfer==0.3.2\n",
      "sacremoses==0.0.43\n",
      "scikit-learn==0.21.3\n",
      "scipy==1.3.1\n",
      "seaborn==0.10.0\n",
      "Send2Trash==1.5.0\n",
      "sentencepiece==0.1.91\n",
      "six==1.12.0\n",
      "smart-open==1.9.0\n",
      "soupsieve==2.0\n",
      "terminado==0.8.2\n",
      "testpath==0.4.2\n",
      "tokenizers==0.8.1rc2\n",
      "torch==1.6.0\n",
      "torchtext==0.7.0\n",
      "torchvision==0.6.0\n",
      "tornado==6.0.3\n",
      "tqdm==4.48.2\n",
      "traitlets==4.3.2\n",
      "transformers==3.3.1\n",
      "urllib3==1.25.6\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "widgetsnbextension==3.5.1\n",
      "wordcloud==1.7.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "# 前処理と単語分割をまとめた関数を作成\n",
    "import re\n",
    "import string\n",
    "from utils.bert import BertTokenizer\n",
    "# フォルダ「utils」のbert.pyより"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のシードを設定\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    '''前処理'''\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    return text\n",
    "\n",
    "\n",
    "# 単語分割用のTokenizerを用意\n",
    "tokenizer_bert = BertTokenizer(\n",
    "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
    "\n",
    "\n",
    "# 前処理と単語分割をまとめた関数を定義\n",
    "# 単語分割の関数を渡すので、tokenizer_bertではなく、tokenizer_bert.tokenizeを渡す点に注意\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)  # tokenizer_bert\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takanokaito/env/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "max_length = 256\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"[CLS]\", eos_token=\"[SEP]\", pad_token='[PAD]', unk_token='[UNK]')\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "ID = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "# (注釈)：各引数を再確認\n",
    "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
    "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
    "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
    "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
    "# include_length: 文章の単語数のデータを保持するか\n",
    "# batch_first：ミニバッチの次元を先頭に用意するかどうか\n",
    "# fix_length：全部の文章を指定した長さと同じになるように、paddingします\n",
    "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takanokaito/env/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/Users/takanokaito/env/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# フォルダ「data」から各csvファイルを読み込みます\n",
    "# BERT用で処理するので、時間がかかります\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='bert_train_data.csv',\n",
    "    test='bert_test_data.csv', format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('id', ID), ('contents', TEXT), ('y', LABEL)])\n",
    "\n",
    "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "train_ds, val_ds = train_val_ds.split(\n",
    "    split_ratio=0.95, random_state=random.seed(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTはBERTが持つ全単語でBertEmbeddingモジュールを作成しているので、ボキャブラリーとしては全単語を使用します\n",
    "# そのため訓練データからボキャブラリーは作成しません\n",
    "\n",
    "# まずBERT用の単語辞書を辞書型変数に用意します\n",
    "from utils.bert import BertTokenizer, load_vocab\n",
    "\n",
    "vocab_bert, ids_to_tokens_bert = load_vocab(\n",
    "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\")\n",
    "\n",
    "\n",
    "# このまま、TEXT.vocab.stoi= vocab_bert (stoiはstring_to_IDで、単語からIDへの辞書)としたいですが、\n",
    "# 一度bulild_vocabを実行しないとTEXTオブジェクトがvocabのメンバ変数をもってくれないです。\n",
    "# （'Field' object has no attribute 'vocab' というエラーをはきます）\n",
    "\n",
    "# 1度適当にbuild_vocabでボキャブラリーを作成してから、BERTのボキャブラリーを上書きします\n",
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "vocab_bert['[unk]'] = vocab_bert['[UNK]']\n",
    "TEXT.vocab.stoi = vocab_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takanokaito/env/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "batch_size = 32  # BERTでは16、32あたりを使用する\n",
    "\n",
    "train_dl = torchtext.data.Iterator(\n",
    "    train_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "val_dl = torchtext.data.Iterator(\n",
    "    val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight→embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight→embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight→embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.gamma→embeddings.LayerNorm.gamma\n",
      "bert.embeddings.LayerNorm.beta→embeddings.LayerNorm.beta\n",
      "bert.encoder.layer.0.attention.self.query.weight→encoder.layer.0.attention.selfattn.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias→encoder.layer.0.attention.selfattn.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight→encoder.layer.0.attention.selfattn.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias→encoder.layer.0.attention.selfattn.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight→encoder.layer.0.attention.selfattn.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias→encoder.layer.0.attention.selfattn.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight→encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias→encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma→encoder.layer.0.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta→encoder.layer.0.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.0.intermediate.dense.weight→encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias→encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight→encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias→encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma→encoder.layer.0.output.LayerNorm.gamma\n",
      "bert.encoder.layer.0.output.LayerNorm.beta→encoder.layer.0.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.attention.self.query.weight→encoder.layer.1.attention.selfattn.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias→encoder.layer.1.attention.selfattn.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight→encoder.layer.1.attention.selfattn.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias→encoder.layer.1.attention.selfattn.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight→encoder.layer.1.attention.selfattn.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias→encoder.layer.1.attention.selfattn.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight→encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias→encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma→encoder.layer.1.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta→encoder.layer.1.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.1.intermediate.dense.weight→encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias→encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight→encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias→encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma→encoder.layer.1.output.LayerNorm.gamma\n",
      "bert.encoder.layer.1.output.LayerNorm.beta→encoder.layer.1.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.attention.self.query.weight→encoder.layer.2.attention.selfattn.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias→encoder.layer.2.attention.selfattn.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight→encoder.layer.2.attention.selfattn.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias→encoder.layer.2.attention.selfattn.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight→encoder.layer.2.attention.selfattn.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias→encoder.layer.2.attention.selfattn.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight→encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias→encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma→encoder.layer.2.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta→encoder.layer.2.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.2.intermediate.dense.weight→encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias→encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight→encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias→encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma→encoder.layer.2.output.LayerNorm.gamma\n",
      "bert.encoder.layer.2.output.LayerNorm.beta→encoder.layer.2.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.attention.self.query.weight→encoder.layer.3.attention.selfattn.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias→encoder.layer.3.attention.selfattn.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight→encoder.layer.3.attention.selfattn.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias→encoder.layer.3.attention.selfattn.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight→encoder.layer.3.attention.selfattn.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias→encoder.layer.3.attention.selfattn.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight→encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias→encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma→encoder.layer.3.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta→encoder.layer.3.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.3.intermediate.dense.weight→encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias→encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight→encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias→encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma→encoder.layer.3.output.LayerNorm.gamma\n",
      "bert.encoder.layer.3.output.LayerNorm.beta→encoder.layer.3.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.attention.self.query.weight→encoder.layer.4.attention.selfattn.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias→encoder.layer.4.attention.selfattn.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight→encoder.layer.4.attention.selfattn.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias→encoder.layer.4.attention.selfattn.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight→encoder.layer.4.attention.selfattn.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias→encoder.layer.4.attention.selfattn.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight→encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias→encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma→encoder.layer.4.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta→encoder.layer.4.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.4.intermediate.dense.weight→encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias→encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight→encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias→encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma→encoder.layer.4.output.LayerNorm.gamma\n",
      "bert.encoder.layer.4.output.LayerNorm.beta→encoder.layer.4.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.attention.self.query.weight→encoder.layer.5.attention.selfattn.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias→encoder.layer.5.attention.selfattn.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight→encoder.layer.5.attention.selfattn.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias→encoder.layer.5.attention.selfattn.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight→encoder.layer.5.attention.selfattn.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias→encoder.layer.5.attention.selfattn.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight→encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias→encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma→encoder.layer.5.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta→encoder.layer.5.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.5.intermediate.dense.weight→encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias→encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight→encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias→encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma→encoder.layer.5.output.LayerNorm.gamma\n",
      "bert.encoder.layer.5.output.LayerNorm.beta→encoder.layer.5.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.attention.self.query.weight→encoder.layer.6.attention.selfattn.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias→encoder.layer.6.attention.selfattn.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight→encoder.layer.6.attention.selfattn.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias→encoder.layer.6.attention.selfattn.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight→encoder.layer.6.attention.selfattn.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias→encoder.layer.6.attention.selfattn.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight→encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias→encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma→encoder.layer.6.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta→encoder.layer.6.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.6.intermediate.dense.weight→encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias→encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight→encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias→encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma→encoder.layer.6.output.LayerNorm.gamma\n",
      "bert.encoder.layer.6.output.LayerNorm.beta→encoder.layer.6.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.attention.self.query.weight→encoder.layer.7.attention.selfattn.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias→encoder.layer.7.attention.selfattn.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight→encoder.layer.7.attention.selfattn.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias→encoder.layer.7.attention.selfattn.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight→encoder.layer.7.attention.selfattn.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias→encoder.layer.7.attention.selfattn.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight→encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias→encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma→encoder.layer.7.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta→encoder.layer.7.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.7.intermediate.dense.weight→encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias→encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight→encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias→encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma→encoder.layer.7.output.LayerNorm.gamma\n",
      "bert.encoder.layer.7.output.LayerNorm.beta→encoder.layer.7.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.attention.self.query.weight→encoder.layer.8.attention.selfattn.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias→encoder.layer.8.attention.selfattn.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight→encoder.layer.8.attention.selfattn.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias→encoder.layer.8.attention.selfattn.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight→encoder.layer.8.attention.selfattn.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias→encoder.layer.8.attention.selfattn.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight→encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias→encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma→encoder.layer.8.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta→encoder.layer.8.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.8.intermediate.dense.weight→encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias→encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight→encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias→encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma→encoder.layer.8.output.LayerNorm.gamma\n",
      "bert.encoder.layer.8.output.LayerNorm.beta→encoder.layer.8.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.attention.self.query.weight→encoder.layer.9.attention.selfattn.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias→encoder.layer.9.attention.selfattn.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight→encoder.layer.9.attention.selfattn.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias→encoder.layer.9.attention.selfattn.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight→encoder.layer.9.attention.selfattn.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias→encoder.layer.9.attention.selfattn.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight→encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias→encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma→encoder.layer.9.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta→encoder.layer.9.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.9.intermediate.dense.weight→encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias→encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight→encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias→encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma→encoder.layer.9.output.LayerNorm.gamma\n",
      "bert.encoder.layer.9.output.LayerNorm.beta→encoder.layer.9.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.attention.self.query.weight→encoder.layer.10.attention.selfattn.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias→encoder.layer.10.attention.selfattn.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight→encoder.layer.10.attention.selfattn.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias→encoder.layer.10.attention.selfattn.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight→encoder.layer.10.attention.selfattn.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias→encoder.layer.10.attention.selfattn.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight→encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias→encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma→encoder.layer.10.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta→encoder.layer.10.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.10.intermediate.dense.weight→encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias→encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight→encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias→encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma→encoder.layer.10.output.LayerNorm.gamma\n",
      "bert.encoder.layer.10.output.LayerNorm.beta→encoder.layer.10.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.attention.self.query.weight→encoder.layer.11.attention.selfattn.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias→encoder.layer.11.attention.selfattn.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight→encoder.layer.11.attention.selfattn.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias→encoder.layer.11.attention.selfattn.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight→encoder.layer.11.attention.selfattn.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias→encoder.layer.11.attention.selfattn.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight→encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias→encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma→encoder.layer.11.attention.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta→encoder.layer.11.attention.output.LayerNorm.beta\n",
      "bert.encoder.layer.11.intermediate.dense.weight→encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias→encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight→encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias→encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma→encoder.layer.11.output.LayerNorm.gamma\n",
      "bert.encoder.layer.11.output.LayerNorm.beta→encoder.layer.11.output.LayerNorm.beta\n",
      "bert.pooler.dense.weight→pooler.dense.weight\n",
      "bert.pooler.dense.bias→pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "from utils.bert import get_config, BertModel, set_learned_params\n",
    "\n",
    "# モデル設定のJOSNファイルをオブジェクト変数として読み込みます\n",
    "config = get_config(file_path=\"./weights/bert_config.json\")\n",
    "\n",
    "# BERTモデルを作成します\n",
    "net_bert = BertModel(config)\n",
    "\n",
    "# BERTモデルに学習済みパラメータセットします\n",
    "net_bert = set_learned_params(\n",
    "    net_bert, weights_path=\"./weights/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class BertForClassify(nn.Module):\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForClassify, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル\n",
    "\n",
    "        # 入力はBERTの出力特徴量の次元、出力は1:Data scientist, 2:Machine learning engineer, 3:Software engineer, 4:Consultantの4つ\n",
    "        self.cls1 = nn.Linear(in_features=768, out_features=64)\n",
    "        self.cls2 = nn.Linear(in_features=64, out_features=2)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls1.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls1.bias, 0)\n",
    "        nn.init.normal_(self.cls2.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls2.bias, 0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # 活性化関数gelu\n",
    "        self.intermediate_act_fn = gelu\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers, pooled_output = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        # 入力文章の1単語目[CLS]の特徴量を使用して、多値分類\n",
    "        vec_0 = encoded_layers[:, 0, :]\n",
    "        vec_0 = self.dropout(vec_0.view(-1, 768))  # sizeを[batch_size, hidden_sizeに変換\n",
    "        hidden = self.cls1(vec_0)\n",
    "        out = self.cls2(self.intermediate_act_fn(self.dropout(hidden)))\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return out, attention_probs, hidden\n",
    "        elif attention_show_flg == False:\n",
    "            return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net = BertForClassify(net_bert)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. 最後のBertLayerモジュールを勾配計算ありに変更\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for name, param in net.cls1.named_parameters():\n",
    "    param.requires_grad = True\n",
    "for name, param in net.cls2.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09009513 0.90990487]\n"
     ]
    }
   ],
   "source": [
    "# GPUが使えるかを確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 最適化手法の設定\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls1.parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls2.parameters(), 'lr': 5e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# 損失関数の設定\n",
    "weights = np.array([0, 0])\n",
    "for example in train_ds:\n",
    "        weights[int(example.y)] += 1\n",
    "weights = 1 / weights\n",
    "weights /= np.sum(weights)\n",
    "print(weights)\n",
    "weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "            \n",
    "            # f1\n",
    "            predlist=torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "            lbllist=torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.contents[0].to(device)  # 文章\n",
    "                labels = batch.y.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BertForClassifyに入力\n",
    "                    outputs, hidden = net(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                  output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data)\n",
    "                                   ).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(\n",
    "                                iteration, loss.item(), duration, acc))\n",
    "                            print(preds[0:10])\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                    # f1\n",
    "                    predlist=torch.cat([predlist,preds.detach().view(-1).cpu()])\n",
    "                    lbllist=torch.cat([lbllist,labels.view(-1).cpu()])\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            # f1\n",
    "            f1 = f1_score(lbllist, predlist)\n",
    "            print('f1_score {:^5}'.format(f1))\n",
    "            t_epoch_start = time.time()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cpu\n",
      "-----start-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takanokaito/env/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション 10 || Loss: 0.6992 || 10iter: 314.6658 sec. || 本イテレーションの正解率：0.21875\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "イテレーション 20 || Loss: 0.6841 || 10iter: 316.3488 sec. || 本イテレーションの正解率：0.53125\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1])\n",
      "イテレーション 30 || Loss: 0.6945 || 10iter: 309.8897 sec. || 本イテレーションの正解率：0.5625\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1])\n",
      "イテレーション 40 || Loss: 0.6443 || 10iter: 308.1357 sec. || 本イテレーションの正解率：0.5625\n",
      "tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "イテレーション 50 || Loss: 0.6574 || 10iter: 311.2739 sec. || 本イテレーションの正解率：0.5\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1])\n",
      "Epoch 1/5 | train |  Loss: 0.6825 Acc: 0.4314\n",
      "f1_score 0.1858974358974359\n",
      "Epoch 1/5 |  val  |  Loss: 0.6598 Acc: 0.6596\n",
      "f1_score 0.30434782608695654\n",
      "イテレーション 10 || Loss: 0.6127 || 10iter: 309.3094 sec. || 本イテレーションの正解率：0.59375\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1])\n",
      "イテレーション 20 || Loss: 0.6042 || 10iter: 307.4083 sec. || 本イテレーションの正解率：0.5\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n",
      "イテレーション 30 || Loss: 0.5008 || 10iter: 309.7979 sec. || 本イテレーションの正解率：0.96875\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "イテレーション 40 || Loss: 0.4990 || 10iter: 311.8554 sec. || 本イテレーションの正解率：0.8125\n",
      "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "イテレーション 50 || Loss: 0.5484 || 10iter: 312.6560 sec. || 本イテレーションの正解率：0.84375\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch 2/5 | train |  Loss: 0.5198 Acc: 0.7879\n",
      "f1_score 0.3955342902711324\n",
      "Epoch 2/5 |  val  |  Loss: 0.3021 Acc: 0.9681\n",
      "f1_score 0.8421052631578948\n",
      "イテレーション 10 || Loss: 0.2054 || 10iter: 314.0214 sec. || 本イテレーションの正解率：0.96875\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "イテレーション 20 || Loss: 0.2310 || 10iter: 301.7984 sec. || 本イテレーションの正解率：0.96875\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0])\n",
      "イテレーション 30 || Loss: 0.1743 || 10iter: 305.6015 sec. || 本イテレーションの正解率：0.9375\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "イテレーション 40 || Loss: 0.1855 || 10iter: 300.2320 sec. || 本イテレーションの正解率：0.96875\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n",
      "イテレーション 50 || Loss: 0.1402 || 10iter: 300.8098 sec. || 本イテレーションの正解率：0.9375\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch 3/5 | train |  Loss: 0.2479 Acc: 0.9407\n",
      "f1_score 0.7389162561576356\n",
      "Epoch 3/5 |  val  |  Loss: 0.1817 Acc: 0.9787\n",
      "f1_score  0.9 \n",
      "イテレーション 10 || Loss: 0.1093 || 10iter: 308.6435 sec. || 本イテレーションの正解率：0.9375\n",
      "tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "イテレーション 20 || Loss: 0.3410 || 10iter: 299.6432 sec. || 本イテレーションの正解率：0.9375\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "イテレーション 30 || Loss: 0.1796 || 10iter: 299.2126 sec. || 本イテレーションの正解率：0.90625\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "イテレーション 40 || Loss: 0.0584 || 10iter: 300.2765 sec. || 本イテレーションの正解率：1.0\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
      "イテレーション 50 || Loss: 0.0743 || 10iter: 309.4848 sec. || 本イテレーションの正解率：0.96875\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "Epoch 4/5 | train |  Loss: 0.1324 Acc: 0.9659\n",
      "f1_score 0.8355795148247979\n",
      "Epoch 4/5 |  val  |  Loss: 0.1379 Acc: 0.9255\n",
      "f1_score 0.7407407407407407\n",
      "イテレーション 10 || Loss: 0.0915 || 10iter: 302.7578 sec. || 本イテレーションの正解率：1.0\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "イテレーション 20 || Loss: 0.0432 || 10iter: 303.4872 sec. || 本イテレーションの正解率：1.0\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "イテレーション 30 || Loss: 0.0363 || 10iter: 305.0603 sec. || 本イテレーションの正解率：1.0\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "イテレーション 40 || Loss: 0.0462 || 10iter: 303.0074 sec. || 本イテレーションの正解率：1.0\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "イテレーション 50 || Loss: 0.1114 || 10iter: 301.9711 sec. || 本イテレーションの正解率：0.96875\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch 5/5 | train |  Loss: 0.0856 Acc: 0.9743\n",
      "f1_score 0.8736263736263736\n",
      "Epoch 5/5 |  val  |  Loss: 0.1774 Acc: 0.9681\n",
      "f1_score 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する。\n",
    "num_epochs = 5\n",
    "net_trained = train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したネットワークパラメータを保存します\n",
    "save_path = './weights/bert_fine_tuning_' + str(num_epochs) + '.pth'\n",
    "torch.save(net_trained.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 777/777 [5:08:09<00:00, 23.80s/it]  \n"
     ]
    }
   ],
   "source": [
    "# テストデータ\n",
    "pred_label_list = torch.zeros(0, dtype=torch.long, device='cpu')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "for batch in tqdm(test_dl):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = batch.contents[0].to(device)  # 文章\n",
    "    # labels = batch.Label.to(device)  # ラベル\n",
    "    \n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # BertForClassifyに入力\n",
    "        outputs, hidden = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
    "                              output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        pred_label_list = torch.cat([pred_label_list,preds.detach().view(-1).cpu()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df  = pd.read_csv('./data/bert_test_data.csv')\n",
    "sub_df['y'] = np.array(pred_label_list)\n",
    "sub_df[['id', 'y']].to_csv('./submit/bert_submit.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>contents</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: re : weather and energy price data\\r\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: organizational study\\r\\ngpg and eott ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: re [ 7 ] : talk about our meds\\r\\nske...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: report about your cable service\\r\\nhi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Subject: start date : 1 / 26 / 02 ; hourahead ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           contents  y\n",
       "0   1  Subject: re : weather and energy price data\\r\\...  0\n",
       "1   2  Subject: organizational study\\r\\ngpg and eott ...  0\n",
       "2   3  Subject: re [ 7 ] : talk about our meds\\r\\nske...  1\n",
       "3   4  Subject: report about your cable service\\r\\nhi...  1\n",
       "4   5  Subject: start date : 1 / 26 / 02 ; hourahead ...  0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15759"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sub_df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
